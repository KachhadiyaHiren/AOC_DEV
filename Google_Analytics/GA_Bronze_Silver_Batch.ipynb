{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17794a0-b417-49c8-b93d-57f8c5fa5d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "BRONZE_TABLE = \"events_raw\"\n",
    "CATALOG = \"dev_aoc_catalog\"\n",
    "BRONZE_SCHEMA = \"bronze_google_analytics\"\n",
    "SK_PADDING = 10\n",
    "\n",
    "bronze_table_full = f\"{CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TABLE}\"\n",
    "\n",
    "# -------------------------------\n",
    "# Silver Table - Sessions (Batch Incremental)\n",
    "# -------------------------------\n",
    "@dlt.table(\n",
    "    name=\"ga_silver_web_sessions\",\n",
    "    comment=\"Silver GA web sessions - Incremental batch processing\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "def silver_sessions():\n",
    "    SK_PREFIX = \"SES\"\n",
    "    \"\"\"\n",
    "    Incremental batch processing of sessions.\n",
    "    DLT automatically tracks what has been processed.\n",
    "    \"\"\"\n",
    "    df = dlt.read(bronze_table_full)\n",
    "    \n",
    "    # Extract fields from STRUCT traffic_source\n",
    "    df = df.withColumn(\"traffic_source_val\", F.col(\"traffic_source.source\")) \\\n",
    "           .withColumn(\"traffic_medium_val\", F.col(\"traffic_source.medium\")) \\\n",
    "           .withColumn(\"traffic_campaign_val\", F.col(\"traffic_source.name\"))\n",
    "    \n",
    "    # Handle potential duplicate columns\n",
    "    if \"user_id\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"user_pseudo_id\", \"user_pseudo_id_sk\")\n",
    "        user_col_name = \"user_pseudo_id_sk\"\n",
    "    else:\n",
    "        df = df.withColumnRenamed(\"user_pseudo_id\", \"user_id\")\n",
    "        user_col_name = \"user_id\"\n",
    "    \n",
    "    # Rename traffic_source fields safely\n",
    "    traffic_source_col = \"traffic_source_extracted\" if \"traffic_source\" in df.columns else \"traffic_source\"\n",
    "    traffic_medium_col = \"traffic_medium_extracted\" if \"traffic_medium\" in df.columns else \"traffic_medium\"\n",
    "    traffic_campaign_col = \"traffic_campaign_extracted\" if \"traffic_campaign\" in df.columns else \"traffic_campaign\"\n",
    "    \n",
    "    df = df.withColumnRenamed(\"traffic_source_val\", traffic_source_col) \\\n",
    "           .withColumnRenamed(\"traffic_medium_val\", traffic_medium_col) \\\n",
    "           .withColumnRenamed(\"traffic_campaign_val\", traffic_campaign_col)\n",
    "    \n",
    "    # Transform columns\n",
    "    df_transformed = (\n",
    "        df\n",
    "        .withColumn(\"session_date\", F.to_date((F.col(\"event_timestamp\")/1000000).cast(\"timestamp\")))\n",
    "        .withColumn(\"session_start_time\", (F.col(\"event_timestamp\")/1000000).cast(\"timestamp\"))\n",
    "        .withColumn(\"device_category\", F.col(\"device.category\"))\n",
    "        .withColumn(\"os\", F.col(\"device.operating_system\"))\n",
    "        .withColumn(\"browser\", F.col(\"device.web_info.browser\"))\n",
    "        .withColumn(\"session_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'ga_session_id')[0].value.int_value\").cast(\"string\"))\n",
    "        .withColumn(\"session_sequence\", F.col(\"event_bundle_sequence_id\"))\n",
    "        .withColumn(\"is_engaged_session\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'session_engaged')[0].value.int_value\").cast(\"int\"))\n",
    "        .withColumn(\"upsert_dttm\", F.current_timestamp())\n",
    "        .withColumn(\"ingestion_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "    try:\n",
    "        max_sk = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(CAST(REGEXP_REPLACE(session_sk, '[^0-9]', '') AS BIGINT)), 0) AS max_num\n",
    "            FROM {CATALOG}.{SILVER_SCHEMA}.ga_silver_web_sessions\n",
    "        \"\"\").collect()[0][\"max_num\"]\n",
    "        )\n",
    "    except:\n",
    "        max_sk = 0\n",
    "    \n",
    "    # Step 7: Assign sequential numbers and generate alphanumeric key\n",
    "    row_window = Window.orderBy(F.col(\"session_start_time\"))\n",
    "    \n",
    "    # Generate deterministic surrogate key\n",
    "    df_with_sk = (\n",
    "        df_transformed\n",
    "        .withColumn(\"temp_row_id\", F.row_number().over(row_window) + max_sk)\n",
    "        .withColumn(\"session_sk\", \n",
    "        F.concat(\n",
    "                F.lit(SK_PREFIX),\n",
    "                F.lpad(F.col(\"temp_row_id\").cast(\"string\"), SK_PADDING, \"0\")\n",
    "            ))\n",
    "        .drop(\"temp_row_id\")\n",
    "        )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_df = df_with_sk.select(\n",
    "        \"session_sk\",\n",
    "        \"session_date\",\n",
    "        user_col_name,\n",
    "        \"session_id\",\n",
    "        traffic_source_col,\n",
    "        traffic_medium_col,\n",
    "        traffic_campaign_col,\n",
    "        \"device_category\",\n",
    "        \"os\",\n",
    "        \"browser\",\n",
    "        \"session_sequence\",\n",
    "        \"session_start_time\",\n",
    "        \"is_engaged_session\",\n",
    "        \"upsert_dttm\",\n",
    "        \"ingestion_id\"\n",
    "    ).withColumnRenamed(user_col_name, \"user_id\") \\\n",
    "     .withColumnRenamed(traffic_source_col, \"traffic_source\") \\\n",
    "     .withColumnRenamed(traffic_medium_col, \"traffic_medium\") \\\n",
    "     .withColumnRenamed(traffic_campaign_col, \"traffic_campaign\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# -------------------------------\n",
    "# Silver Table - Form Fills (Batch Incremental)\n",
    "# -------------------------------\n",
    "@dlt.table(\n",
    "    name=\"ga_silver_form_fill\",\n",
    "    comment=\"Silver GA form submissions - Incremental batch processing\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_form_event\", \"event_name = 'form_submit'\")\n",
    "def silver_form_fill():\n",
    "    SK_PREFIX = \"FF\"\n",
    "    \"\"\"\n",
    "    Incremental batch processing of form submission events.\n",
    "    \"\"\"\n",
    "    df = dlt.read(bronze_table_full)\n",
    "    \n",
    "    # Filter only form_submit events\n",
    "    df_forms = df.filter(F.col(\"event_name\") == \"form_submit\")\n",
    "    \n",
    "    # Extract form-specific parameters\n",
    "    df_transformed = (\n",
    "        df_forms\n",
    "        .withColumn(\"form_date\", F.to_date((F.col(\"event_timestamp\")/1000000).cast(\"timestamp\")))\n",
    "        .withColumn(\"form_submit_time\", (F.col(\"event_timestamp\")/1000000).cast(\"timestamp\"))\n",
    "        .withColumn(\"session_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'ga_session_id')[0].value.int_value\").cast(\"string\"))\n",
    "        .withColumn(\"form_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'form_id')[0].value.string_value\"))\n",
    "        .withColumn(\"form_name\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'form_name')[0].value.string_value\"))\n",
    "        .withColumn(\"form_page_url\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'page_location')[0].value.string_value\"))\n",
    "        .withColumn(\"form_type\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'form_type')[0].value.string_value\"))\n",
    "        .withColumn(\"insert_dttm\", F.current_timestamp())\n",
    "        .withColumn(\"ingestion_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        max_sk = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(CAST(REGEXP_REPLACE(form_fill_sk, '[^0-9]', '') AS BIGINT)), 0) AS max_num\n",
    "            FROM {CATALOG}.{SILVER_SCHEMA}.ga_silver_form_fill\n",
    "        \"\"\").collect()[0][\"max_num\"]\n",
    "        )\n",
    "    except:\n",
    "        max_sk = 0\n",
    "    \n",
    "    # Step 7: Assign sequential numbers and generate alphanumeric key\n",
    "    row_window = Window.orderBy(F.col(\"form_submit_time\"))\n",
    "    \n",
    "    # Generate deterministic surrogate key\n",
    "    df_with_sk = (\n",
    "        df_transformed\n",
    "        .withColumn(\"temp_row_id\", F.row_number().over(row_window) + max_sk)\n",
    "        .withColumn(\"form_fill_sk\", \n",
    "        F.concat(\n",
    "                F.lit(SK_PREFIX),\n",
    "                F.lpad(F.col(\"temp_row_id\").cast(\"string\"), SK_PADDING, \"0\")\n",
    "            ))\n",
    "        .drop(\"temp_row_id\")\n",
    "        )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_df = df_with_sk.select(\n",
    "        \"form_fill_sk\",\n",
    "        \"form_date\",\n",
    "        F.col(\"user_pseudo_id\").alias(\"user_id\"),\n",
    "        \"session_id\",\n",
    "        \"form_submit_time\",\n",
    "        \"event_name\",\n",
    "        \"form_id\",\n",
    "        \"form_name\",\n",
    "        \"form_page_url\",\n",
    "        \"form_type\",\n",
    "        \"insert_dttm\",\n",
    "        \"ingestion_id\"\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# -------------------------------\n",
    "# Silver Table - Web Events (Batch Incremental)\n",
    "# -------------------------------\n",
    "@dlt.table(\n",
    "    name=\"ga_silver_web_event\",\n",
    "    comment=\"Silver GA web events - All event types - Incremental batch processing\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "def silver_web_event():\n",
    "    SK_PREFIX = \"WE\"\n",
    "    \"\"\"\n",
    "    Incremental batch processing of all web events.\n",
    "    Captures all event types with key parameters.\n",
    "    \"\"\"\n",
    "    df = dlt.read(bronze_table_full)\n",
    "    \n",
    "    # Extract common event parameters\n",
    "    df_transformed = (\n",
    "        df\n",
    "        .withColumn(\"event_date\", F.to_date((F.col(\"event_timestamp\")/1000000).cast(\"timestamp\")))\n",
    "        .withColumn(\"session_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'ga_session_id')[0].value.int_value\").cast(\"string\"))\n",
    "        .withColumn(\"ga_session_number\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'ga_session_number')[0].value.int_value\").cast(\"int\"))\n",
    "        .withColumn(\"engagement_time_msec\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'engagement_time_msec')[0].value.int_value\").cast(\"bigint\"))\n",
    "        .withColumn(\"insert_dttm\", F.current_timestamp())\n",
    "        .withColumn(\"ingestion_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        max_sk = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(CAST(REGEXP_REPLACE(web_event_sk, '[^0-9]', '') AS BIGINT)), 0) AS max_num\n",
    "            FROM {CATALOG}.{SILVER_SCHEMA}.ga_silver_web_event\n",
    "        \"\"\").collect()[0][\"max_num\"]\n",
    "        )\n",
    "    except:\n",
    "        max_sk = 0\n",
    "    \n",
    "    # Step 7: Assign sequential numbers and generate alphanumeric key\n",
    "    row_window = Window.orderBy(F.col(\"engagement_time_msec\"))\n",
    "    \n",
    "    # Generate deterministic surrogate key\n",
    "    df_with_sk = (\n",
    "        df_transformed\n",
    "        .withColumn(\"temp_row_id\", F.row_number().over(row_window) + max_sk)\n",
    "        .withColumn(\"web_event_sk\", \n",
    "        F.concat(\n",
    "                F.lit(SK_PREFIX),\n",
    "                F.lpad(F.col(\"temp_row_id\").cast(\"string\"), SK_PADDING, \"0\")\n",
    "            ))\n",
    "        .drop(\"temp_row_id\")\n",
    "        )\n",
    "    \n",
    "    # Create event_id (business key) - combination of user + timestamp + event\n",
    "    df_with_bk = df_with_sk.withColumn(\n",
    "        \"event_id\",\n",
    "        F.concat_ws(\"_\", \n",
    "            F.col(\"user_pseudo_id\"),\n",
    "            F.col(\"event_timestamp\").cast(\"string\"),\n",
    "            F.col(\"event_name\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_df = df_with_bk.select(\n",
    "        \"web_event_sk\",\n",
    "        \"event_id\",\n",
    "        \"event_date\",\n",
    "        \"event_name\",\n",
    "        \"event_bundle_sequence_id\",\n",
    "        F.col(\"user_id\").alias(\"user_id\"),  # CRM/login ID if available\n",
    "        \"session_id\",\n",
    "        \"ga_session_number\",\n",
    "        \"engagement_time_msec\",\n",
    "        \"insert_dttm\",\n",
    "        \"ingestion_id\"\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# -------------------------------\n",
    "# Silver Table - Purchases (Batch Incremental)\n",
    "# -------------------------------\n",
    "@dlt.table(\n",
    "    name=\"ga_silver_purchase\",\n",
    "    comment=\"Silver GA purchases - Incremental batch processing\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "# @dlt.expect_or_drop(\"valid_transaction_id\", \"transaction_id IS NOT NULL\")\n",
    "# @dlt.expect_or_drop(\"valid_product\", \"product_id IS NOT NULL\")\n",
    "# @dlt.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "# @dlt.expect_or_drop(\"valid_price\", \"price IS NOT NULL AND price >= 0\")\n",
    "# @dlt.expect_or_drop(\"valid_currency\", \"currency IS NOT NULL\")\n",
    "def silver_purchase():\n",
    "    SK_PREFIX = \"PUR\"\n",
    "    \"\"\"\n",
    "    Incremental batch processing of purchase events.\n",
    "    Explodes items array to get individual product purchases.\n",
    "    \"\"\"\n",
    "    df = dlt.read(bronze_table_full)\n",
    "    \n",
    "    # Filter only purchase events\n",
    "    # df_purchases = df.filter(F.col(\"event_name\") == \"purchase\")\n",
    "    \n",
    "    # Explode items array to get individual products\n",
    "    df_exploded = df.withColumn(\"item\", F.explode_outer(F.col(\"items\")))\n",
    "    \n",
    "    # Extract purchase and item details\n",
    "    df_transformed = (\n",
    "        df_exploded\n",
    "        .withColumn(\"purchase_date\", F.to_date((F.col(\"event_timestamp\")/1000000).cast(\"timestamp\")))\n",
    "        .withColumn(\"session_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'ga_session_id')[0].value.int_value\").cast(\"string\"))\n",
    "        .withColumn(\"transaction_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'transaction_id')[0].value.string_value\"))\n",
    "        .withColumn(\"currency\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'currency')[0].value.string_value\"))\n",
    "        .withColumn(\"product_id\", F.col(\"item.item_id\"))\n",
    "        .withColumn(\"product_name\", F.col(\"item.item_name\"))\n",
    "        .withColumn(\"item_variant\", F.col(\"item.item_variant\"))\n",
    "        .withColumn(\"price\", F.col(\"item.price\"))\n",
    "        .withColumn(\"quantity\", F.col(\"item.quantity\"))\n",
    "        .withColumn(\"insert_dttm\", F.current_timestamp())\n",
    "        .withColumn(\"ingestion_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        max_sk = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(CAST(REGEXP_REPLACE(purchase_sk, '[^0-9]', '') AS BIGINT)), 0) AS max_num\n",
    "            FROM {CATALOG}.{SILVER_SCHEMA}.ga_silver_purchase\n",
    "        \"\"\").collect()[0][\"max_num\"]\n",
    "        )\n",
    "    except:\n",
    "        max_sk = 0\n",
    "    \n",
    "    # Step 7: Assign sequential numbers and generate alphanumeric key\n",
    "    row_window = Window.orderBy(F.col(\"purchase_date\"))\n",
    "    \n",
    "    # Generate deterministic surrogate key\n",
    "    df_with_sk = (\n",
    "        df_transformed\n",
    "        .withColumn(\"temp_row_id\", F.row_number().over(row_window) + max_sk)\n",
    "        .withColumn(\"purchase_sk\", \n",
    "        F.concat(\n",
    "                F.lit(SK_PREFIX),\n",
    "                F.lpad(F.col(\"temp_row_id\").cast(\"string\"), SK_PADDING, \"0\")\n",
    "            ))\n",
    "        .drop(\"temp_row_id\")\n",
    "        )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_df = df_with_sk.select(\n",
    "        \"purchase_sk\",\n",
    "        \"transaction_id\",\n",
    "        F.col(\"user_pseudo_id\").alias(\"user_id\"),\n",
    "        \"session_id\",\n",
    "        \"product_id\",\n",
    "        \"purchase_date\",\n",
    "        \"currency\",\n",
    "        \"product_name\",\n",
    "        \"item_variant\",\n",
    "        \"price\",\n",
    "        \"quantity\",\n",
    "        \"insert_dttm\",\n",
    "        \"ingestion_id\"\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# -------------------------------\n",
    "# Silver Table - Page Views (Batch Incremental)\n",
    "# -------------------------------\n",
    "@dlt.table(\n",
    "    name=\"ga_silver_page_views\",\n",
    "    comment=\"Silver GA page views - Incremental batch processing\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_page_view_event\", \"event_name = 'page_view'\")\n",
    "def silver_page_views():\n",
    "    SK_PREFIX = \"PV\"\n",
    "    \"\"\"\n",
    "    Incremental batch processing of page view events.\n",
    "    DLT automatically tracks what has been processed.\n",
    "    \"\"\"\n",
    "    df = dlt.read(bronze_table_full)\n",
    "    \n",
    "    # Filter only page_view events\n",
    "    df_page_views = df.filter(F.col(\"event_name\") == \"page_view\")\n",
    "    \n",
    "    # Extract page view specific parameters from event_params array\n",
    "    df_transformed = (\n",
    "        df_page_views\n",
    "        .withColumn(\"view_date\", F.to_date((F.col(\"event_timestamp\")/1000000).cast(\"timestamp\")))\n",
    "        .withColumn(\"view_timestamp\", (F.col(\"event_timestamp\")/1000000).cast(\"timestamp\"))\n",
    "        .withColumn(\"session_id\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'ga_session_id')[0].value.int_value\").cast(\"string\"))\n",
    "        .withColumn(\"page_url\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'page_location')[0].value.string_value\"))\n",
    "        .withColumn(\"page_title\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'page_title')[0].value.string_value\"))\n",
    "        .withColumn(\"referrer_url\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'page_referrer')[0].value.string_value\"))\n",
    "        .withColumn(\"engagement_time\", \n",
    "                    F.expr(\"filter(event_params, x -> x.key = 'engagement_time_msec')[0].value.int_value\").cast(\"bigint\"))\n",
    "        .withColumn(\"insert_dttm\", F.current_timestamp())\n",
    "        .withColumn(\"ingestion_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        max_sk = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(CAST(REGEXP_REPLACE(page_view_sk, '[^0-9]', '') AS BIGINT)), 0) AS max_num\n",
    "            FROM {CATALOG}.{SILVER_SCHEMA}.ga_silver_page_views\n",
    "        \"\"\").collect()[0][\"max_num\"]\n",
    "        )\n",
    "    except:\n",
    "        max_sk = 0\n",
    "    \n",
    "    # Step 7: Assign sequential numbers and generate alphanumeric key\n",
    "    row_window = Window.orderBy(F.col(\"view_timestamp\"))\n",
    "    \n",
    "    # Generate deterministic surrogate key\n",
    "    df_with_sk = (\n",
    "        df_transformed\n",
    "        .withColumn(\"temp_row_id\", F.row_number().over(row_window) + max_sk)\n",
    "        .withColumn(\"page_view_sk\", \n",
    "        F.concat(\n",
    "                F.lit(SK_PREFIX),\n",
    "                F.lpad(F.col(\"temp_row_id\").cast(\"string\"), SK_PADDING, \"0\")\n",
    "            ))\n",
    "        .drop(\"temp_row_id\")\n",
    "        )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_df = df_with_sk.select(\n",
    "        \"page_view_sk\",\n",
    "        \"view_date\",\n",
    "        F.col(\"user_pseudo_id\").alias(\"user_id\"),\n",
    "        \"session_id\",\n",
    "        \"view_timestamp\",\n",
    "        \"event_name\",\n",
    "        \"page_url\",\n",
    "        \"page_title\",\n",
    "        \"referrer_url\",\n",
    "        \"engagement_time\",\n",
    "        \"insert_dttm\",\n",
    "        \"ingestion_id\"\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# -------------------------------\n",
    "# Silver Table - Users (Batch Incremental)\n",
    "# -------------------------------\n",
    "@dlt.table(\n",
    "    name=\"ga_silver_web_users\",\n",
    "    comment=\"Silver GA web users - Incremental batch processing\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "def silver_users():\n",
    "    SK_PREFIX = \"WU\"\n",
    "    \"\"\"\n",
    "    Incremental batch processing of users.\n",
    "    DLT automatically tracks what has been processed.\n",
    "    \"\"\"\n",
    "    df = dlt.read(bronze_table_full)\n",
    "    \n",
    "    # Explode user_properties array\n",
    "    df_exploded = df.withColumn(\"user_prop\", F.explode_outer(F.col(\"user_properties\"))) \\\n",
    "                    .withColumn(\"user_property_name\", F.col(\"user_prop.key\")) \\\n",
    "                    .withColumn(\"user_property_value\", F.coalesce(\n",
    "                        F.col(\"user_prop.value.string_value\"),\n",
    "                        F.col(\"user_prop.value.int_value\").cast(\"string\"),\n",
    "                        F.col(\"user_prop.value.float_value\").cast(\"string\"),\n",
    "                        F.col(\"user_prop.value.double_value\").cast(\"string\")\n",
    "                    ))\n",
    "    \n",
    "    # Aggregate at user level with window functions\n",
    "    window_first = Window.partitionBy(\"user_pseudo_id\").orderBy(F.col(\"event_timestamp\").asc())\n",
    "    \n",
    "    df_users = (\n",
    "        df_exploded\n",
    "        .withColumn(\"first_touch_medium\", F.first(F.col(\"traffic_source.medium\")).over(window_first))\n",
    "        .withColumn(\"first_touch_source\", F.first(F.col(\"traffic_source.source\")).over(window_first))\n",
    "        .withColumn(\"first_touch_campaign\", F.first(F.col(\"traffic_source.name\")).over(window_first))\n",
    "        .groupBy(\"user_pseudo_id\")\n",
    "        .agg(\n",
    "            F.first(\"user_id\").alias(\"user_id\"),\n",
    "            F.min(\"user_first_touch_timestamp\").alias(\"first_seen_time\"),\n",
    "            F.first(\"device.category\").alias(\"preferred_device\"),\n",
    "            F.first(\"geo.country\").alias(\"home_country\"),\n",
    "            F.first(\"geo.city\").alias(\"home_city\"),\n",
    "            F.first(\"first_touch_medium\").alias(\"first_touch_medium\"),\n",
    "            F.first(\"first_touch_source\").alias(\"first_touch_source\"),\n",
    "            F.first(\"first_touch_campaign\").alias(\"first_touch_campaign\"),\n",
    "            F.first(\"user_property_name\").alias(\"user_property_name\"),\n",
    "            F.first(\"user_property_value\").alias(\"user_property_value\")\n",
    "        )\n",
    "        .withColumn(\"identified_user_flg\", \n",
    "                    F.when(F.col(\"user_id\").isNotNull(), F.lit(\"Known\"))\n",
    "                     .otherwise(F.lit(\"Unknown\")))\n",
    "        .withColumn(\"start_dttm\", F.current_timestamp())\n",
    "        .withColumn(\"end_dttm\", F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"ingestion_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "    \n",
    "    # Create surrogate key\n",
    "    try:\n",
    "        max_sk = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(CAST(REGEXP_REPLACE(user_sk, '[^0-9]', '') AS BIGINT)), 0) AS max_num\n",
    "            FROM {CATALOG}.{SILVER_SCHEMA}.ga_silver_web_users\n",
    "        \"\"\").collect()[0][\"max_num\"]\n",
    "        )\n",
    "    except:\n",
    "        max_sk = 0\n",
    "    \n",
    "    # Step 7: Assign sequential numbers and generate alphanumeric key\n",
    "    row_window = Window.orderBy(F.col(\"start_dttm\"))\n",
    "    \n",
    "    # Generate deterministic surrogate key\n",
    "    df_with_sk = (\n",
    "        df_users\n",
    "        .withColumn(\"temp_row_id\", F.row_number().over(row_window) + max_sk)\n",
    "        .withColumn(\"user_sk\", \n",
    "        F.concat(\n",
    "                F.lit(SK_PREFIX),\n",
    "                F.lpad(F.col(\"temp_row_id\").cast(\"string\"), SK_PADDING, \"0\")\n",
    "            ))\n",
    "        .drop(\"temp_row_id\")\n",
    "        )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_df = df_with_sk.select(\n",
    "        \"user_sk\",\n",
    "        \"user_id\",\n",
    "        F.col(\"user_pseudo_id\").alias(\"pseudo_id\"),\n",
    "        \"first_seen_time\",\n",
    "        \"preferred_device\",\n",
    "        \"home_country\",\n",
    "        \"home_city\",\n",
    "        \"first_touch_medium\",\n",
    "        \"first_touch_source\",\n",
    "        \"first_touch_campaign\",\n",
    "        \"user_property_name\",\n",
    "        \"user_property_value\",\n",
    "        \"identified_user_flg\",\n",
    "        \"start_dttm\",\n",
    "        \"end_dttm\",\n",
    "        \"ingestion_id\"\n",
    "    )\n",
    "    \n",
    "    return final_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GA_Bronze_Silver_Batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
